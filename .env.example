# NestJS Template Environment Variables

# General
NODE_ENV=development
APP_NAME=command-centre-api
APP_VERSION=1.0.0

# Observability (@paystackhq/nestjs-observability) and Langfuse
# Service identification
OTEL_SERVICE_NAME=command-centre-api
OTEL_SERVICE_VERSION=1.0.0
OTEL_SERVICE_ENV=local

# Logging
LOG_LEVEL=info
USE_JSON_LOGGER=true
DEBUG=false

# OpenTelemetry Exporters (none, console, otlp)
OTEL_LOGS_EXPORTER=console
OTEL_TRACES_EXPORTER=console
OTEL_METRICS_EXPORTER=console

# OTLP Endpoints (only needed if using otlp exporters)
OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://localhost:4318/v1/logs
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces
OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://localhost:4318/v1/metrics

# Metrics
OTEL_METRICS_ENABLED=true
OTEL_METRICS_ENDPOINT=/metrics

# Tracing
OTEL_TRACES_SAMPLER=always_on
OTEL_TRACES_SAMPLER_ARG=1.0
OTEL_SPAN_ATTRIBUTE_SANITIZATION_ENABLED=true

# Database Configuration
DATABASE_HOST=mongodb
DATABASE_USERNAME=root
DATABASE_PASSWORD=root
DATABASE_NAME=command-centre-api

# Redis Configuration (for chart data caching)
REDIS_READ_URL=redis://redis:6379
REDIS_WRITE_URL=redis://redis:6379
REDIS_USERNAME=
REDIS_PASSWORD=
REDIS_DB=0
CACHE_TTL=10800000  # 3 hours in milliseconds (3 * 60 * 60 * 1000)

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Paystack Configuration
# Base URL for Paystack API (defaults to staging if not set)
# Production: https://api.paystack.co
# Staging: https://studio-api.paystack.co
PAYSTACK_API_BASE_URL=https://studio-api.paystack.co

# Rate Limiting
MESSAGE_LIMIT=100
RATE_LIMIT_PERIOD_HOURS=24
MESSAGE_HISTORY_LIMIT=40
CONVERSATION_TTL_DAYS=3

# JWT Authentication
JWT_SECRET=your-secret-key-change-in-production
JWT_EXPIRES_IN=24h

# Conversation Summarization
MAX_SUMMARIES=2                     # Maximum summaries before closing conversation
CONTEXT_WINDOW_SIZE=128000          # Model context window (gpt-4o-mini default: 128k tokens)
TOKEN_THRESHOLD_PERCENTAGE=0.6      # Trigger summarization at 60% of context window

# Langfuse LLM Observability (Optional)
# For LLM tracing, monitoring, and analytics
LANGFUSE_ENABLED=false                                          # Enable Langfuse observability (set to true to enable)
LANGFUSE_PUBLIC_KEY=                                            # Langfuse public key (pk-lf-...)
LANGFUSE_SECRET_KEY=                                            # Langfuse secret key (sk-lf-...)
LANGFUSE_BASE_URL=https://cloud.langfuse.com                    # Langfuse API URL (defaults to cloud)
LANGFUSE_FLUSH_INTERVAL=5000                                    # Flush interval in milliseconds (default: 5000)
LANGFUSE_FLUSH_AT=15                                            # Flush after N spans (default: 15)
LANGFUSE_FILTER_VERBOSE_METADATA=true                           # Filter verbose OTEL metadata from spans (default: true)

# OpenTelemetry Span Processor Hook (Required for Langfuse integration)
# Uncomment the line below when enabling Langfuse
# OTEL_SPAN_PROCESSORS_PATH=./dist/common/ai/observability/langfuse.config.js
